{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-05T13:20:34.905828Z",
     "iopub.status.busy": "2025-09-05T13:20:34.905497Z",
     "iopub.status.idle": "2025-09-05T13:20:35.173707Z",
     "shell.execute_reply": "2025-09-05T13:20:35.172902Z",
     "shell.execute_reply.started": "2025-09-05T13:20:34.905802Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 此 Python 3 环境已预装了许多实用的分析库。\n",
    "# 它由kaggle/python Docker定义 image: https://github.com/kaggle/docker-python\n",
    "# 例如，以下是几个有用的包供加载：\n",
    "\n",
    "import numpy as np # 线性代数\n",
    "import pandas as pd # 数据处理，csv格式\n",
    "\n",
    "# 输入数据文件位于只读目录\"../input/\"中\n",
    "# 例如，运行此代码（通过点击运行或按下Shift+Enter）将列出输入目录下的所有文件\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# 您可以将最多20GB的文件写入当前目录（/kaggle/working/），这些文件在创建版本时会被保留为输出。\n",
    "# 您还可以将临时文件写入/kaggle/temp/，但它们不会在当前会话之外保存。\n",
    "# 例如，以下是将数据写入/kaggle/working/的代码：\n",
    "# with open('/kaggle/working/my_file.txt', 'w') as f:\n",
    "#     f.write('Hello, world!')\n",
    "# 您可以在/kaggle/working/中找到my_file.txt文件。\n",
    "# 您还可以在/kaggle/temp/中找到临时文件。\n",
    "# 您可以在/kaggle/working/和/kaggle/temp/中找到的文件在创建版本时会被保留为输出。\n",
    "# 您可以在/kaggle/working/和/kaggle/temp/中找到的文件在当前会话之外不会被保存。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T13:20:35.175101Z",
     "iopub.status.busy": "2025-09-05T13:20:35.174770Z",
     "iopub.status.idle": "2025-09-05T13:20:40.539853Z",
     "shell.execute_reply": "2025-09-05T13:20:40.538891Z",
     "shell.execute_reply.started": "2025-09-05T13:20:35.175082Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in /opt/anaconda3/lib/python3.12/site-packages (2.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (3.9.5)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (5.9.0)\n",
      "Requirement already satisfied: pyparsing in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (3.0.9)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (2.32.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from torch_geometric) (4.66.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.9.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch_geometric) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->torch_geometric) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T13:20:40.541311Z",
     "iopub.status.busy": "2025-09-05T13:20:40.541009Z",
     "iopub.status.idle": "2025-09-05T13:20:50.529249Z",
     "shell.execute_reply": "2025-09-05T13:20:50.528592Z",
     "shell.execute_reply.started": "2025-09-05T13:20:40.541284Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 0, 0, 1, 2],\n",
      "        [1, 2, 0, 0, 1, 2]])\n",
      "tensor([[0, 0, 1, 1, 2],\n",
      "        [0, 1, 1, 2, 2]])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid, Amazon, Reddit, WikiCS, Flickr, WebKB, Actor, PolBlogs, CitationFull\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import AddSelfLoops\n",
    "from torch_geometric.utils import subgraph, k_hop_subgraph\n",
    "import torch_geometric.transforms as T\n",
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch_geometric.nn import global_add_pool, global_max_pool, GlobalAttention, global_mean_pool\n",
    "from torch_geometric.nn.inits import glorot\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "edge_index = torch.tensor([[0, 1, 0],\n",
    "                           [1, 2, 0]], dtype=torch.long)\n",
    "x = torch.tensor([[1,2,3],\n",
    "                 [4,5,6],\n",
    "                 [6,7,8]])\n",
    "y = torch.tensor([0,1,2])\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "# 创建 AddSelfLoops 转换\n",
    "add_self_loops = AddSelfLoops()\n",
    "# 应用转换\n",
    "data = add_self_loops(data)\n",
    "print(data.edge_index)\n",
    "data.edge_index = torch.unique(data.edge_index, dim=1)\n",
    "print(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T13:20:50.531468Z",
     "iopub.status.busy": "2025-09-05T13:20:50.531052Z",
     "iopub.status.idle": "2025-09-05T13:20:50.662444Z",
     "shell.execute_reply": "2025-09-05T13:20:50.661579Z",
     "shell.execute_reply.started": "2025-09-05T13:20:50.531448Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Zhuanz/Desktop/MetaPrompt\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export http_proxy=http://127.0.0.1:7890\n",
    "!export https_proxy=http://127.0.0.1:7890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T13:20:50.663825Z",
     "iopub.status.busy": "2025-09-05T13:20:50.663572Z",
     "iopub.status.idle": "2025-09-05T13:21:01.225417Z",
     "shell.execute_reply": "2025-09-05T13:21:01.224580Z",
     "shell.execute_reply.started": "2025-09-05T13:20:50.663803Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m transform_list \u001b[38;5;241m=\u001b[39m [T\u001b[38;5;241m.\u001b[39mAddSelfLoops(), T\u001b[38;5;241m.\u001b[39mToUndirected(), T\u001b[38;5;241m.\u001b[39mNormalizeFeatures()]\n\u001b[1;32m     48\u001b[0m transform \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mCompose(transform_list)\n\u001b[0;32m---> 49\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Planetoid(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCora\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[1;32m     50\u001b[0m data \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     51\u001b[0m graph_list \u001b[38;5;241m=\u001b[39m induced_graphs(data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch_geometric/datasets/planetoid.py:102\u001b[0m, in \u001b[0;36mPlanetoid.__init__\u001b[0;34m(self, root, name, split, num_train_per_class, num_val, num_test, transform, pre_transform, force_reload)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m=\u001b[39m split\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublic\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeom-gcn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform, pre_transform,\n\u001b[1;32m    103\u001b[0m                  force_reload\u001b[38;5;241m=\u001b[39mforce_reload)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_paths[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch_geometric/data/in_memory_dataset.py:81\u001b[0m, in \u001b[0;36mInMemoryDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     74\u001b[0m     root: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m     force_reload: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     80\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform, pre_transform, pre_filter, log,\n\u001b[1;32m     82\u001b[0m                      force_reload)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data: Optional[BaseData] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslices: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch_geometric/data/dataset.py:112\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforce_reload \u001b[38;5;241m=\u001b[39m force_reload\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_download:\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download()\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_process:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch_geometric/data/dataset.py:229\u001b[0m, in \u001b[0;36mDataset._download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    228\u001b[0m fs\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownload()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch_geometric/datasets/planetoid.py:154\u001b[0m, in \u001b[0;36mPlanetoid.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_file_names:\n\u001b[0;32m--> 154\u001b[0m         fs\u001b[38;5;241m.\u001b[39mcp(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_dir)\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeom-gcn\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch_geometric/io/fs.py:114\u001b[0m, in \u001b[0;36mcp\u001b[0;34m(path1, path2, extract, log, use_cache, clear_cache)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcp\u001b[39m(\n\u001b[1;32m    105\u001b[0m     path1: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    106\u001b[0m     path2: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m     clear_cache: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    111\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     kwargs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 114\u001b[0m     is_path1_dir \u001b[38;5;241m=\u001b[39m isdir(path1)\n\u001b[1;32m    115\u001b[0m     is_path2_dir \u001b[38;5;241m=\u001b[39m isdir(path2)\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Cache result if the protocol is not local:\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch_geometric/io/fs.py:62\u001b[0m, in \u001b[0;36misdir\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misdir\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_fs(path)\u001b[38;5;241m.\u001b[39misdir(path)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/fsspec/asyn.py:118\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop, func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/fsspec/asyn.py:91\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m asyncio\u001b[38;5;241m.\u001b[39mrun_coroutine_threadsafe(_runner(event, coro, result, timeout), loop)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m# this loops allows thread to get interrupted\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m event\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def induced_graphs(data, device, smallest_size=10, largest_size=30):   # 构建诱导图的过程\n",
    "    induced_graph_list = []\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    for index in range(data.x.size(0)):\n",
    "        current_label = data.y[index].item()\n",
    "\n",
    "        current_hop = 2\n",
    "        subset, _, _, _ = k_hop_subgraph(node_idx=index, num_hops=current_hop,\n",
    "                                            edge_index=data.edge_index, relabel_nodes=True)\n",
    "        subset = subset\n",
    "\n",
    "        while len(subset) < smallest_size and current_hop < 5:\n",
    "            current_hop += 1\n",
    "            subset, _, _, _ = k_hop_subgraph(node_idx=index, num_hops=current_hop,\n",
    "                                                edge_index=data.edge_index)\n",
    "            \n",
    "        if len(subset) < smallest_size:\n",
    "            need_node_num = smallest_size - len(subset)\n",
    "            pos_nodes = torch.argwhere(data.y == int(current_label))   # Test data may leak\n",
    "            pos_nodes = pos_nodes.to('cpu')\n",
    "            subset = subset.to('cpu')\n",
    "            candidate_nodes = torch.from_numpy(np.setdiff1d(pos_nodes.numpy(), subset.numpy()))\n",
    "            candidate_nodes = candidate_nodes[torch.randperm(candidate_nodes.shape[0])][0:need_node_num]\n",
    "            subset = torch.cat([torch.flatten(subset), torch.flatten(candidate_nodes)])\n",
    "\n",
    "        if len(subset) > largest_size:\n",
    "            subset = subset[torch.randperm(subset.shape[0])][0:largest_size - 1]\n",
    "            subset = torch.unique(torch.cat([torch.LongTensor([index]).to(device), torch.flatten(subset).to(device)]))\n",
    "\n",
    "        subset = subset.to(device)\n",
    "        sub_edge_index, _ = subgraph(subset, data.edge_index, relabel_nodes=True)\n",
    "        sub_edge_index = sub_edge_index.to(device)\n",
    "\n",
    "        x = data.x[subset]\n",
    "\n",
    "        induced_graph = Data(x=x, edge_index=sub_edge_index, y=data.y[index], index = index)\n",
    "        add_self_loops = AddSelfLoops()\n",
    "        induced_graph = add_self_loops(induced_graph)\n",
    "        induced_graph.edge_index = torch.unique(induced_graph.edge_index, dim=1)\n",
    "        induced_graph_list.append(induced_graph)\n",
    "        if index%500 == 0:\n",
    "            print(index)\n",
    "    return induced_graph_list\n",
    "\n",
    "\n",
    "transform_list = [T.AddSelfLoops(), T.ToUndirected(), T.NormalizeFeatures()]\n",
    "transform = T.Compose(transform_list)\n",
    "dataset = Planetoid(root='./data', name='Cora', transform=transform)\n",
    "data = dataset[0]\n",
    "graph_list = induced_graphs(data, 'cpu')\n",
    "print(graph_list[0],graph_list[0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T13:21:01.226791Z",
     "iopub.status.busy": "2025-09-05T13:21:01.226225Z",
     "iopub.status.idle": "2025-09-05T13:21:01.383748Z",
     "shell.execute_reply": "2025-09-05T13:21:01.383108Z",
     "shell.execute_reply.started": "2025-09-05T13:21:01.226758Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_adj_tensor(adj):\n",
    "    # gcn的归一化邻接矩阵方法\n",
    "    D = torch.sum(adj, dim=1)\n",
    "    D_inv = torch.pow(D, -1 / 2)\n",
    "    D_inv[torch.isinf(D_inv)] = 0.\n",
    "    D_mat_inv = torch.diag(D_inv)\n",
    "    adj_norm = D_mat_inv @ adj @ D_mat_inv  # GCN的归一化方式\n",
    "    return adj_norm\n",
    "\n",
    "def edge_index_to_adjacency_matrix(edge_index, num_nodes, undirected=True, device='cpu'):  \n",
    "    # 将edge_indedx转化为邻接矩阵\n",
    "    # 构建一个大小为 (num_nodes, num_nodes) 的零矩阵  \n",
    "    adjacency_matrix = torch.zeros(num_nodes, num_nodes, dtype=torch.uint8).to(device)\n",
    "      \n",
    "    # 使用索引广播机制，一次性将边索引映射到邻接矩阵的相应位置上  \n",
    "    if undirected:\n",
    "        adjacency_matrix[edge_index[0], edge_index[1]] = 1  \n",
    "        adjacency_matrix[edge_index[1], edge_index[0]] = 1  \n",
    "    else:\n",
    "        adjacency_matrix[edge_index[0], edge_index[1]] = 1\n",
    "    return adjacency_matrix\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "size_buffer = []\n",
    "for graph in graph_list:\n",
    "    size_buffer.append(graph.x.shape[0])\n",
    "    adj = edge_index_to_adjacency_matrix(graph.edge_index, graph.x.shape[0])\n",
    "    graph.adj = adj\n",
    "\n",
    "print(graph_list[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T13:21:01.384837Z",
     "iopub.status.busy": "2025-09-05T13:21:01.384534Z",
     "iopub.status.idle": "2025-09-05T13:21:01.394604Z",
     "shell.execute_reply": "2025-09-05T13:21:01.393841Z",
     "shell.execute_reply.started": "2025-09-05T13:21:01.384809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, graph_list):\n",
    "        self.graph_list = graph_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graph_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graph_list[idx]\n",
    "\n",
    "class SequentialGraphLoader(DataLoader):\n",
    "    def __init__(self, graph_list, batch_size=1):\n",
    "        dataset = GraphDataset(graph_list)\n",
    "        super(SequentialGraphLoader, self).__init__(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.dataset), self.batch_size):\n",
    "            batch = self.dataset[i:i+self.batch_size]\n",
    "            yield self.merge_graphs(batch)\n",
    "\n",
    "    def merge_graphs(self, batch):\n",
    "        # 合并子图的特征矩阵和邻接矩阵\n",
    "        x_buffer = []\n",
    "        adj_list = []\n",
    "        y_buffer = []\n",
    "        batch_sizes = []\n",
    "        batch_indexs = []\n",
    "        graph_indexs = []\n",
    "\n",
    "        for i, graph in enumerate(batch):\n",
    "            x = graph.x\n",
    "            adj = graph.adj\n",
    "            y = graph.y.unsqueeze(0)  # 确保 y 是二维张量\n",
    "            x_buffer.append(x)\n",
    "            adj_list.append(adj)\n",
    "            y_buffer.append(y)\n",
    "            batch_sizes.append(x.shape[0])  # 存储子图的大小\n",
    "            batch_indexs = batch_indexs + [i] * x.shape[0]\n",
    "            graph_indexs.append(graph.index)\n",
    "\n",
    "        # 将特征矩阵堆叠\n",
    "        x_combined = torch.cat(x_buffer, dim=0)  # 合并特征矩阵\n",
    "        num_nodes = x_combined.size(0)  # 所有节点的总数\n",
    "        \n",
    "        # 构建大的邻接矩阵\n",
    "        adj_combined = torch.zeros(num_nodes, num_nodes, device=x_combined.device)\n",
    "        start = 0\n",
    "        for i in range(len(batch)):\n",
    "            end = start + batch_sizes[i]\n",
    "            adj_combined[start:end, start:end] = adj_list[i]  # 填充对应的子图邻接矩阵\n",
    "            start = end\n",
    "\n",
    "        y_combined = torch.cat(y_buffer, dim=0)  # 合并标签\n",
    "        batch_indexs = torch.tensor(batch_indexs)\n",
    "\n",
    "        return x_combined, adj_combined, y_combined, batch_sizes, batch_indexs, graph_indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T13:21:01.396107Z",
     "iopub.status.busy": "2025-09-05T13:21:01.395431Z",
     "iopub.status.idle": "2025-09-05T13:21:06.522402Z",
     "shell.execute_reply": "2025-09-05T13:21:06.521691Z",
     "shell.execute_reply.started": "2025-09-05T13:21:01.396078Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m modified_graph_list \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(graph_list)\n\u001b[1;32m      8\u001b[0m adj_changes \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mFloatTensor(size, size)) \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m size_buffer] \u001b[38;5;66;03m# 对每个子图的扰动\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m adj_change \u001b[38;5;129;01min\u001b[39;00m adj_changes:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'graph_list' is not defined"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Batch, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import math\n",
    "import copy\n",
    "\n",
    "modified_graph_list = copy.deepcopy(graph_list)\n",
    "adj_changes = [torch.nn.Parameter(torch.FloatTensor(size, size)) for size in size_buffer] # 对每个子图的扰动\n",
    "for adj_change in adj_changes:\n",
    "    adj_change.data.fill_(0)\n",
    "for modified_graph, graph, adj_change in zip(modified_graph_list, graph_list, adj_changes):\n",
    "    change_square = adj_change - torch.diag(torch.diag(adj_change, 0))\n",
    "    change_square = torch.clamp(change_square, -1, 1)\n",
    "    modified_graph.adj = change_square + graph.adj\n",
    "\n",
    "train_loader = SequentialGraphLoader(modified_graph_list[:800], batch_size=16) # 定义的Dataloader\n",
    "test_loader = SequentialGraphLoader(modified_graph_list[800:], batch_size=16)\n",
    "weights = []\n",
    "w_velocities = []\n",
    "hidden_sizes = [128 for i in range(2)]\n",
    "previous_size = 1433\n",
    "out_dim = 7\n",
    "for ix, nhid in enumerate(hidden_sizes):\n",
    "    weight = torch.nn.Parameter(torch.FloatTensor(previous_size, nhid).to(device))\n",
    "    w_velocity = torch.zeros(weight.shape).to(device)\n",
    "    weights.append(weight)\n",
    "    w_velocities.append(w_velocity)\n",
    "    previous_size = nhid\n",
    "    \n",
    "output_weight = torch.nn.Parameter(torch.FloatTensor(previous_size, out_dim).to(device))\n",
    "output_w_velocity = torch.zeros(output_weight.shape).to(device)\n",
    "weights.append(output_weight)\n",
    "w_velocities.append(output_w_velocity)\n",
    "for w, v in zip(weights, w_velocities):\n",
    "    stdv = 1. / math.sqrt(w.size(1))\n",
    "    w.data.uniform_(-stdv, stdv)\n",
    "    v.data.fill_(0)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # 分类任务的损失函数\n",
    "\n",
    "\n",
    "#初始化参数,避免梯度爆炸\n",
    "for w, v in zip(weights, w_velocities):\n",
    "    stdv = 1. / math.sqrt(w.size(1))\n",
    "    w.data.uniform_(-stdv, stdv)\n",
    "    v.data.fill_(0)\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    print(\"epoch_num:\", epoch)\n",
    "    for x, adj, y, sizes, batch_indexs, graph_indexs in train_loader:\n",
    "        loss = 0.0\n",
    "        x, adj, y, batch_indexs = x.to(device), adj.to(device), y.to(device), batch_indexs.to(device)\n",
    "        adj_norm = normalize_adj_tensor(adj)\n",
    "        \n",
    "        # 图卷积\n",
    "        for i, w in enumerate(weights):\n",
    "            if i != len(weights)-1:\n",
    "                x = adj_norm @ x @ w\n",
    "            else:\n",
    "                x = global_mean_pool(x, batch_indexs) @ w\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = criterion(x, y)\n",
    "        assert torch.isnan(loss).any()==False, \"loss is NaN\"\n",
    "        if torch.isnan(loss).any():\n",
    "            raise ValueError(\"loss is NaN!\")\n",
    "        weight_grads = torch.autograd.grad(loss, weights, create_graph=True)\n",
    "        w_velocities = [0.9 * v + g for v, g in zip(w_velocities, weight_grads)]\n",
    "        weights = [w - 0.01 * v for w, v in zip(weights, w_velocities)]\n",
    "        \n",
    "total_loss = 0.0\n",
    "for x, adj, y, sizes, batch_indexs, graph_indexs in test_loader:\n",
    "    x, adj, y, batch_indexs = x.to(device), adj.to(device), y.to(device), batch_indexs.to(device)\n",
    "    adj_norm = normalize_adj_tensor(adj)\n",
    "    \n",
    "    # 图卷积\n",
    "    for i, w in enumerate(weights):\n",
    "        if i != len(weights)-1:\n",
    "            x = adj_norm @ x @ w\n",
    "        else:\n",
    "            x = global_mean_pool(x, batch_indexs) @ w\n",
    "        \n",
    "    # 累计损失\n",
    "    total_loss += criterion(x, y) * x.shape[0]\n",
    "\n",
    "total_loss.backward(retain_graph=False)\n",
    "\n",
    "# 提取元梯度\n",
    "adj_grads = [adj_change.grad for adj_change in adj_changes]\n",
    "\n",
    "# 验证形状\n",
    "for grad, adj_change in zip(adj_grads, adj_changes):\n",
    "    assert grad.shape == adj_change.shape\n",
    "print(\"done\")\n",
    "print(adj_grads[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-05T13:21:06.523533Z",
     "iopub.status.busy": "2025-09-05T13:21:06.523211Z",
     "iopub.status.idle": "2025-09-05T13:21:06.527251Z",
     "shell.execute_reply": "2025-09-05T13:21:06.526636Z",
     "shell.execute_reply.started": "2025-09-05T13:21:06.523506Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#在上述实验中可以发现需要较小的batch_size以支持meta-gradient的计算(e.g., batch_size=16); 如果太大则会出现Out-of-Memory, 因此在这里记录一下梯度图空间复杂度的理论分析：\n",
    "\n",
    "#假设现在有m个子图，平均每个图有n个节点，那么对于一个epoch来说，有m/batch_size个batch;\n",
    "\n",
    "#由于需要拼接整个batch的子图成一个大图，那么这个大图的邻接矩阵规模为 (n * batch_size)^2\n",
    "\n",
    "#综上, 存储的梯度图理论空间复杂度为：\n",
    "#O(m/batch_size*(n*batch_size)^2) = O(m*n^2*batch_size)\n",
    "\n",
    "#进一步考虑到完整的surrogate_training有k个epoch,那么整个surrogate_training的空间复杂度为：\n",
    "#O(k*m*n^2*batch_size)\n",
    "\n",
    "#因此过大的batch_size必然导致过大的内存开销\n",
    "#但如果batch_size过小, 比如极端情况下batch_size=1, 那么理论上讲导致surrogate_training过程过于“琐碎”, 无法感知到训练的全局方向。（TODO, 需要进一步做一个实验）\n",
    "#上面问题更新:目前部分实验做下来batch_size不会特别影响surrogate_training的效果,但batch_size过小将导致更长的开销时间（TODO, 再进一步做一个实验）\n",
    "\n",
    "#现在分析node_injection的理论空间复杂度:\n",
    "#进一步假设注入的节点个数为原图个数的b%,那么一共有m*n*b%个注入节点,每个注入节点只考虑添加到一个子图,因此优化节点注入位置的拓扑有n条,那么需要元梯度的拓扑扰动总数为m*n^2*b%\n",
    "#如果同时也考虑优化节点的d维特征,那么需要元梯度的特征总数为m*n*d*b%\n",
    "#整个surrogate_training的空间复杂度为:\n",
    "#O(k*m*n*(n+d)*b%)\n",
    "\n",
    "#分析O(k*m*n^2*batch_size)和O(k*m*n*(n+d)*b%)大小, 即比较n*batch_size和（n+d）*b%\n",
    "#往往为了不同任务域迁移,需要PCA降维,d不会过大，如100维\n",
    "#因此显然 n*batch_size >>（n+d）*b%\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
